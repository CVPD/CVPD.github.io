@article{alirezazadeh2024mises,
  title={Mises-Fisher similarity-based boosted additive angular margin loss for breast cancer classification},
  author={Alirezazadeh, Pendar and Dornaika, Fadi and Charafeddine, Jinan},
  journal={Artificial Intelligence Review},
  volume={57},
  number={12},
  pages={326},
  year={2024},
  publisher={Springer},
  doi={10.1007/s10462-024-10963-4},
  url={https://link.springer.com/article/10.1007/s10462-024-10963-4},
  abstract={To enhance the accuracy of breast cancer diagnosis, current practices rely on biopsies and microscopic examinations. However, this approach is known for being time-consuming, tedious, and costly. While convolutional neural networks (CNNs) have shown promise for their efficiency and high accuracy, training them effectively becomes challenging in real-world learning scenarios such as class imbalance, small-scale datasets, and label noises. Angular margin-based softmax losses, which concentrate on the angle between features and classifiers embedded in cosine similarity at the classification layer, aim to regulate feature representation learning. Nevertheless, the cosine similarity's lack of a heavy tail impedes its ability to compactly regulate intra-class feature distribution, limiting generalization performance. Moreover, these losses are constrained to target classes when margin penalties are applied, which may not always optimize effectiveness. Addressing these hurdles, we introduce an innovative approach termed MF-BAM (Mises-Fisher Similarity-based Boosted Additive Angular Margin Loss), which extends beyond traditional cosine similarity and is anchored in the von Mises-Fisher distribution. MF-BAM not only penalizes the angle between deep features and their corresponding target class weights but also considers angles between deep features and weights associated with non-target classes. Through extensive experimentation on the BreaKHis dataset, MF-BAM achieves outstanding accuracies of 99.92%, 99.96%, 100.00%, and 98.05% for magnification levels of x40, x100, x200, and x400, respectively. Furthermore, additional experiments conducted on the BACH dataset for breast cancer classification, as well as on the LFW and YTF datasets for face recognition, affirm the generalization capability of our proposed loss function.}
}

@article{alirezazadeh2024discriminative,
  author = {Alirezazadeh, Pendar and Dornaika, Fadi and Moujahid, Abdelmalik},
  title = {Discriminative Feature Learning Through Angular Margin-Based Softmax Losses for Breast Cancer Classification},
  journal = {Biophysical Reviews and Letters},
  volume = {19},
  number = {04},
  pages = {291-311},
  year = {2024},
  doi = {10.1142/S1793048024400022},
  URL = {https://doi.org/10.1142/S1793048024400022},
  eprint = {https://doi.org/10.1142/S1793048024400022},
  abstract = { When many histopathological breast images with different magnification levels need to be analyzed, diagnosing benign or malignant cancer from the images can be time-consuming. Automatic classification of histopathological images of breast cancer can support the diagnostic workflow in pathology, reducing analysis time. Recently, convolutional neural networks (CNNs) have been used for more accurate classification of breast cancer histopathological images. CNNs typically highlight semantic information to extract discriminative features. However, the traditional softmax loss used by CNNs usually lacks sufficient discriminative power. To address this problem, several angular margin-based softmax loss functions have been proposed, including Large Margin Softmax Loss (A-Softmax), Large Margin Cosine Loss (CosFace), Additive Angular Margin Loss (ArcFace), and Linear ArcFace (Li-ArcFace). All of these improved losses are based on the same concept: maximizing inter-class variance while minimizing intra-class distance. This paper focuses on these four losses and their effectiveness in extracting discriminative features and creating decision margins between classes. Extensive experimental evaluations were conducted on a public and well-known histopathological breast cancer image dataset (BreakHis). Further experiments with the BACH dataset for breast cancer classification and the SARS-CoV-2 CT-scan dataset for COVID-19 detection affirm the generalization capability of angular margin-based softmax losses in medical image classification.}
}

@article{alirezazadeh2023boosted,
title = {Boosted Additive Angular Margin Loss for breast cancer diagnosis from histopathological images},
journal = {Computers in Biology and Medicine},
volume = {166},
pages = {107528},
year = {2023},
issn = {0010-4825},
doi = {10.1016/j.compbiomed.2023.107528},
url = {https://www.sciencedirect.com/science/article/pii/S0010482523009939},
author = {Pendar Alirezazadeh and Fadi Dornaika},
abstract = {Pathologists use biopsies and microscopic examination to accurately diagnose breast cancer. This process is time-consuming, labor-intensive, and costly. Convolutional neural networks (CNNs) offer an efficient and highly accurate approach to reduce analysis time and automate the diagnostic workflow in pathology. However, the softmax loss commonly used in existing CNNs leads to noticeable ambiguity in decision boundaries and lacks a clear constraint for minimizing within-class variance. In response to this problem, a solution in the form of softmax losses based on angular margin was developed. These losses were introduced in the context of face recognition, with the goal of integrating an angular margin into the softmax loss. This integration improves discrimination features during CNN training by effectively increasing the distance between different classes while reducing the variance within each class. Despite significant progress, these losses are limited to target classes only when margin penalties are applied, which may not lead to optimal effectiveness. In this paper, we introduce Boosted Additive Angular Margin Loss (BAM) to obtain highly discriminative features for breast cancer diagnosis from histopathological images. BAM not only penalizes the angle between deep features and their target class weights, but also considers angles between deep features and non-target class weights. We performed extensive experiments on the publicly available BreaKHis dataset. BAM achieved remarkable accuracies of 99.79%, 99.86%, 99.96%, and 97.65% for magnification levels of 40X, 100X, 200X, and 400X, respectively. These results show an improvement in accuracy of 0.13%, 0.34%, and 0.21% for 40X, 100X, and 200X magnifications, respectively, compared to the baseline methods. Additional experiments were performed on the BACH dataset for breast cancer classification and on the widely accepted LFW and YTF datasets for face recognition to evaluate the generalization ability of the proposed loss function. The results show that BAM outperforms state-of-the-art methods by increasing the decision space between classes and minimizing intra-class variance, resulting in improved discriminability.}
}

@Article{alirezazadeh2023chasing,
  AUTHOR = {Alirezazadeh, Pendar and Dornaika, Fadi and Moujahid, Abdelmalik},
  TITLE = {Chasing a Better Decision Margin for Discriminative Histopathological Breast Cancer Image Classification},
  JOURNAL = {Electronics},
  VOLUME = {12},
  YEAR = {2023},
  NUMBER = {20},
  ARTICLE-NUMBER = {4356},
  URL = {https://www.mdpi.com/2079-9292/12/20/4356},
  ISSN = {2079-9292},
  ABSTRACT = {When considering a large dataset of histopathologic breast images captured at various magnification levels, the process of distinguishing between benign and malignant cancer from these images can be time-intensive. The automation of histopathological breast cancer image classification holds significant promise for expediting pathology diagnoses and reducing the analysis time. Convolutional neural networks (CNNs) have recently gained traction for their ability to more accurately classify histopathological breast cancer images. CNNs excel at extracting distinctive features that emphasize semantic information. However, traditional CNNs employing the softmax loss function often struggle to achieve the necessary discriminatory power for this task. To address this challenge, a set of angular margin-based softmax loss functions have emerged, including angular softmax (A-Softmax), large margin cosine loss (CosFace), and additive angular margin (ArcFace), each sharing a common objective: maximizing inter-class variation while minimizing intra-class variation. This study delves into these three loss functions and their potential to extract distinguishing features while expanding the decision boundary between classes. Rigorous experimentation on a well-established histopathological breast cancer image dataset, BreakHis, has been conducted. As per the results, it is evident that CosFace focuses on augmenting the differences between classes, while A-Softmax and ArcFace tend to emphasize augmenting within-class variations. These observations underscore the efficacy of margin penalties on angular softmax losses in enhancing feature discrimination within the embedding space. These loss functions consistently outperform softmax-based techniques, either by widening the gaps among classes or enhancing the compactness of individual classes.},
  DOI = {10.3390/electronics12204356}
}

@article{alirezazadeh2022adeep,
  title = {A deep learning loss based on additive cosine margin: Application to fashion style and face recognition},
  journal = {Applied Soft Computing},
  volume = {131},
  pages = {109776},
  year = {2022},
  issn = {1568-4946},
  doi = {10.1016/j.asoc.2022.109776},
  url = {https://www.sciencedirect.com/science/article/pii/S1568494622008250},
  author = {Pendar Alirezazadeh and Fadi Dornaika and Abdelmalik Moujahid},
  abstract = {Recently, loss functions based on angular spans improved the performance of deep visual recognition. These losses converted Euclidean cross entropy to angular cross entropy loss. Fashion style recognition deals with the problem of assigning a person's outfit to a fashion style category. Due to the high similarity between different clothing items and the use of softmax-based loss functions, many of the current methods that address this problem show relatively poor performance and cannot guarantee sufficient inter-class margins in the fashion domain. In this work, we propose an end-to-end method for deep visual recognition by combining a standard CNN architecture with a novel loss function, which we call Additive Cosine Margin Loss (ACML). The proposed function not only projects feature vectors of different classes into different regions of the embedding, but also enforces compactness of the projections within each class. Our experiments were conducted on two public and well-known fashion style recognition datasets FashionStyle14 and HipsterWars, and on the face verification and identification datasets LFW, YTF, and MegaFace. These experiments demonstrate the superiority of the proposed loss function over: (1) existing angular margin-based loss functions (2) state-of-the-art methods for clothing style recognition as well as face analysis tasks.}
}

@Article{alirezazadeh2022deep,
  AUTHOR = {Alirezazadeh, Pendar and Dornaika, Fadi and Moujahid, Abdelmalik},
  TITLE = {Deep Learning with Discriminative Margin Loss for Cross-Domain Consumer-to-Shop Clothes Retrieval},
  JOURNAL = {Sensors},
  VOLUME = {22},
  YEAR = {2022},
  NUMBER = {7},
  ARTICLE-NUMBER = {2660},
  URL = {https://www.mdpi.com/1424-8220/22/7/2660},
  PubMedID = {35408276},
  ISSN = {1424-8220},
  ABSTRACT = {Consumer-to-shop clothes retrieval refers to the problem of matching photos taken by customers with their counterparts in the shop. Due to some problems, such as a large number of clothing categories, different appearances of clothing items due to different camera angles and shooting conditions, different background environments, and different body postures, the retrieval accuracy of traditional consumer-to-shop models is always low. With advances in convolutional neural networks (CNNs), the accuracy of garment retrieval has been significantly improved. Most approaches addressing this problem use single CNNs in conjunction with a softmax loss function to extract discriminative features. In the fashion domain, negative pairs can have small or large visual differences that make it difficult to minimize intraclass variance and maximize interclass variance with softmax. Margin-based softmax losses such as Additive Margin-Softmax (aka CosFace) improve the discriminative power of the original softmax loss, but since they consider the same margin for the positive and negative pairs, they are not suitable for cross-domain fashion search. In this work, we introduce the cross-domain discriminative margin loss (DML) to deal with the large variability of negative pairs in fashion. DML learns two different margins for positive and negative pairs such that the negative margin is larger than the positive margin, which provides stronger intraclass reduction for negative pairs. The experiments conducted on publicly available fashion datasets DARN and two benchmarks of the DeepFashion dataset—(1) Consumer-to-Shop Clothes Retrieval and (2) InShop Clothes Retrieval—confirm that the proposed loss function not only outperforms the existing loss functions but also achieves the best performance.},
  DOI = {10.3390/s22072660}
}