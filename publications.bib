@article{dornaika2025towards,
  title={Towards Dynamic Self-Training for Scalable Semi-Supervised Learning on Graphs},
  author={Fadi Dornaika and Zoulfikar Ibrahim and Jinan Charafeddine and Alireza Bosaghzadeh},
  journal={Neurocomputing},
  volume = {654},
  pages={131223},
  year={2025},
  publisher={Elsevier},
  issn = {0925-2312},
  doi={10.1016/j.neucom.2025.131223},
  url={https://www.sciencedirect.com/science/article/pii/S0925231225018958},
  abstract={In the realm of graph-based semi-supervised learning (GSSL), traditional methodologies often struggle to effectively handle labeled samples and scale to accommodate large datasets. To increase supervision information in semi-supervised learning, the self-training paradigm is often used, mainly in datasets with moderate sizes. On the other hand, the use of anchors was adopted with large datasets. In this research endeavor, we propose a novel framework for GSSL that leverages a novel self-training principle tailored for very large datasets, and introduces an advanced method for automatic graph construction using anchors. Our approach focuses on utilizing generated labels of random batches of unlabeled samples, subsequently incorporating these predictions into the training set to enhance the model's accuracy. Pseudo-labeling, a specific instance of self-training, assigns pseudo-labels to the most confidently predicted unlabeled examples, treating them as ground truth during the training phase. By constructing anchor-to-anchor affinity graphs that incorporate both feature and label information, our method facilitates robust learning on large-scale datasets. Through comprehensive experimentation across diverse large datasets, our approach demonstrates its efficacy in achieving scalable and reliable semi-supervised learning outcomes. These findings represent a significant advancement in the field of GSSL, with wide-ranging implications for various applications across different domains. Our method not only addresses the scalability issue but also ensures the effective integration of both labeled and pseudo labeled data, thereby enhancing the overall learning process.},
  keywords = {Graph-based semi-supervised learning, Self-training, Pseudo-labels, Automatic graph construction, Reduced flexible manifold embedding},
}
@article{peng2025multi,
  title={Multi-view learning with graph convolution networks adopting diverse graphs and genuine deep feature fusion},
  author={Guowen Peng and Fadi Dornaika and Jinan Charafeddine},
  journal={Artificial Intelligence Review},
  volume={58},
  number={9},
  pages={1--27},
  year={2025},
  publisher={Springer},
  url={https://link.springer.com/article/10.1007/s10462-025-11301-y},
  doi={10.1007/s10462-025-11301-y},
  abstract={Multi-view data significantly enhances the accuracy of machine learning algorithms by providing a comprehensive representation of object features. Despite their potential, research on the use of Graph Convolutional Networks (GCNs) for processing node connectivity and data features remains limited. Existing methods mainly focus on weighted summation of graph matrices, with only a few approaches effectively integrating the feature information into the graph structures. To overcome these limitations, this paper proposes a novel deep learning architecture: the Feature Fusion and Multi-Graph Fusion Learning Framework (MGCN-FN). The framework consists of two core modules: Feature Fusion Network (FFN): Designed to extract and consolidate key features from multiple views. Multi-Graph Fusion Network (MGFN): Constructs multiple graphs for each view and jointly optimizes both the graph weights and the GCN model. Extensive experiments on various multi-view datasets show that MGCN-FN achieves superior performance compared to state-of-the-art methods, especially on semi-supervised multi-view classification tasks.},
}

@article{maroun2025integrating,
  title={Integrating ConvNeXt and vision transformers for enhancing facial age estimation},
  author={Maroun, Gaby and Bekhouche, Salah Eddine and Charafeddine, Jinan and Dornaika, Fadi},
  journal={Computer Vision and Image Understanding},
  pages={104542},
  year={2025},
  publisher={Elsevier},
  url={https://www.sciencedirect.com/science/article/pii/S1077314225002656},
  doi={10.1016/j.cviu.2025.104542},
  abstract={Age estimation from facial images is a complex and multifaceted challenge in computer vision. In this study, we present a novel hybrid architecture that combines ConvNeXt, a state-of-the-art advancement of convolutional neural networks (CNNs), with Vision Transformers (ViT). While each model independently delivers excellent performance on a variety of tasks, their integration leverages the complementary strengths of the CNNs' localized feature extraction capabilities and the Transformers' global attention mechanisms.
  Our proposed ConvNeXt-ViT hybrid solution was thoroughly evaluated on benchmark age estimation datasets, including MORPH II, CACD, and AFAD, and achieved superior performance in terms of mean absolute error (MAE). To address computational constraints, we leverage pre-trained models and systematically explore different configurations, using linear layers and advanced regularization techniques to optimize the architecture.
  Comprehensive ablation studies highlight the critical role of individual components and training strategies, and in particular emphasize the importance of adapted attention mechanisms within the CNN framework to improve the model's focus on age-relevant facial features. The results show that the ConvNeXt-ViT hybrid not only outperforms traditional methods, but also provides a robust foundation for future advances in age estimation and related visual tasks.
  This work underscores the transformative potential of hybrid architectures and represents a promising direction for the seamless integration of CNNs and transformers to address complex computer vision challenges.},
}

@article{moujahid2025advanced,
  title={Advanced unsupervised learning: a comprehensive overview of multi-view clustering techniques},
  author={Moujahid, Abdelmalik and Dornaika, Fadi},
  journal={Artificial Intelligence Review},
  volume={58},
  number={8},
  pages={234},
  year={2025},
  publisher={Springer},
  url={https://link.springer.com/article/10.1007/s10462-025-11240-8},
  doi={10.1007/s10462-025-11240-8},
  abstract={Machine learning techniques face numerous challenges to achieve optimal performance. These include computational constraints, the limitations of single-view learning algorithms and the complexity of processing large datasets from different domains, sources or views. In this context, multi-view clustering (MVC), a class of unsupervised multi-view learning, emerges as a powerful approach to overcome these challenges. MVC compensates for the shortcomings of single-view methods and provides a richer data representation and effective solutions for a variety of unsupervised learning tasks. In contrast to traditional single-view approaches, the semantically rich nature of multi-view data increases its practical utility despite its inherent complexity. This survey makes a threefold contribution: (1) a systematic categorization of multi-view clustering methods into well-defined groups, including co-training, co-regularization, subspace, deep learning, kernel-based, anchor-based, and graph-based strategies; (2) an in-depth analysis of their respective strengths, weaknesses, and practical challenges, such as scalability and incomplete data; and (3) a forward-looking discussion of emerging trends, interdisciplinary applications, and future directions in MVC research. This study represents an extensive workload, encompassing the review of over 140 foundational and recent publications, the development of comparative insights on integration strategies such as early fusion, late fusion, and joint learning, and the structured investigation of practical use cases in the areas of healthcare, multimedia, and social network analysis. By integrating these efforts, this work aims to fill existing gaps in MVC research and provide actionable insights for the advancement of the field.},
}