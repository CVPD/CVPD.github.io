---
title: Learning Gaze-aware Compositional GAN from Limited Annotations
authors:
- Nerea Aranjuelo
- Siyu Huang
- Ignacio Arganda-Carreras
- Luis Unzueta
- Oihana Otaegui
- Hanspeter Pfister
- Donglai Wei
date: '2024-01-01'
publishDate: '2025-05-30T15:30:23.942954Z'
publication_types:
- conference-paper
publication: '*Proceedings of the ACM on Computer Graphics and Interactive Techniques*'
doi: 10.1145/3654706
abstract: Gaze-annotated facial data is crucial for training deep neural networks
  (DNNs) for gaze estimation. However, obtaining these data is labor-intensive and
  requires specialized equipment due to the challenge of accurately annotating the
  gaze direction of a subject. In this work, we present a generative framework to
  create annotated gaze data by leveraging the benefits of labeled and unlabeled data
  sources. We propose a Gaze-aware Compositional GAN that learns to generate annotated
  facial images from a limited labeled dataset. Then we transfer this model to an
  unlabeled data domain to take advantage of the diversity it provides. Experiments
  demonstrate our approach's effectiveness in generating within-domain image augmentations
  in the ETH-XGaze dataset and cross-domain augmentations in the CelebAMask-HQ dataset
  domain for gaze estimation DNN training. We also show additional applications of
  our work, which include facial image editing and gaze redirection.
---
